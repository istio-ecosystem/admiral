
## Install

### Prerequisites

* One or more k8s clusters with version 1.13 or above
* [Install istio control plane](https://istio.io/docs/setup/install/multicluster/gateways/#deploy-the-istio-control-plane-in-each-cluster) on each of these k8s clusters
* [Configure DNS redirect](https://istio.io/docs/setup/install/multicluster/gateways/#setup-dns) for entries ending in `global`
* Remove envoy cluster rewrite filter
Delete Istio's envoy filter for translating `global` to `svc.cluster.local` at istio-ingressgateway because we don't need that as Admiral generates Service Entries for cross cluster communication to just work!
```
    # Delete envoy filter for translating `global` to `svc.cluster.local`
    kubectl delete envoyfilter istio-multicluster-ingressgateway -n istio-system
```

`Reference:` [K8s cluster installed with Istio_replicated control planes](https://istio.io/docs/setup/install/multicluster/gateways/#deploy-the-istio-control-plane-in-each-cluster)


## Example Installations & Demos

### Single cluster

#### Install/Run Admiral

```
#Download and extract admiral

wget https://github.com/istio-ecosystem/admiral/releases/download/v0.9/admiral-install-v0.9.tar.gz
tar xvf admiral-install-v0.9.tar.gz

export ADMIRAL_HOME=./admiral-install-v0.9
```

```
#Install admiral
$ADMIRAL_HOME/scripts/install_admiral.sh $ADMIRAL_HOME

```

```
#Create the secret for admiral to monitor.

#Since this is for a single cluster demo the remote and local context are the same
$ADMIRAL_HOME/scripts/cluster-secret.sh $KUBECONFIG  $KUBECONFIG admiral
```
```
#Verify the secret
kubectl get secrets -n admiral
```

#### Deploy Sample Services

```
#Install test services & verify admiral did it's magic

$ADMIRAL_HOME/scripts/install_sample_services.sh $ADMIRAL_HOME

```

#### Demo

Now, run the command below that uses the CNAME generated by Admiral
```
kubectl exec --namespace=sample -it $(kubectl get pod -l "app=webapp" --namespace=sample -o jsonpath='{.items[0].metadata.name}') -c webapp -- curl -v http://default.greeting.global
```

#### Generated configuration

Admiral generated Istio configuration.  

##### ServiceEntry

Two service entries were created in the `admiral-sync` namespace.

```kubectl get ServiceEntry -n admiral-sync```

```
NAME                      HOSTS                    LOCATION        RESOLUTION   AGE
default.greeting.global-se   [default.greeting.global]   MESH_INTERNAL   DNS          76m
default.webapp.global-se   [default.webapp.global]   MESH_INTERNAL   DNS          76m
```

```kubectl get ServiceEntry default.greeting.global-se  -n admiral-sync -o yaml```

Looking in more detail the hostname default.greeting.global is pointing back the default k8s FQDNs

```
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  creationTimestamp: "2019-09-20T22:04:59Z"
  generation: 1
  labels:
    identity: greeting
  name: default.greeting.global-se
  namespace: admiral-sync
  resourceVersion: "452814"
  selfLink: /apis/networking.istio.io/v1alpha3/namespaces/admiral-sync/serviceentries/default.greeting.global-se
  uid: b02cdbee-dbf2-11e9-9461-0aa9b467cf9c
spec:
  addresses:
  - 127.0.10.2
  endpoints:
  - address: greeting.sample.svc.cluster.local
    locality: us-west-2
    ports:
      http: 80
  hosts:
  - default.greeting.global
  location: MESH_INTERNAL
  ports:
  - name: http
    number: 80
    protocol: http
  resolution: DNS
```


### Multicluster

Finish steps from Single Cluster to have Admiral running and ready to watch other clusters (lets call them remote clusters) which we will be setting in the steps below.

Let's call the cluster used in Single cluster set up `Cluster 1`. Now we will use the steps below to add `Cluster 2` to the mesh and have it monitored by Admiral

Finish the steps from `Prerequisites` section for `Cluster 2`

#### Add Cluster 2 to Admiral's watcher
```
# Set CLUSTER_1 env variable
export CLUSTER_1=<path_to_kubeconfig_for_cluster_1>

# Set CLUSTER_2 env variable
export CLUSTER_2=<path_to_kubeconfig_for_cluster_2>
```

```
# Switch kubectx to Cluster 2
export KUBECONFIG=$CLUSTER_2
# Create admiral role and bindings on Cluster 2
kubectl apply -f $ADMIRAL_HOME/yaml/remotecluster.yaml
```

```
#Switch kubectx to Cluster 1
export KUBECONFIG=$CLUSTER_1

# Create the k8s secret for admiral to monitor Cluster 2.
$ADMIRAL_HOME/scripts/cluster-secret.sh $CLUSTER_1 $CLUSTER_2 admiral
```

At this point, admiral is watching `Cluster 2`

#### Deploy Sample Services in Cluster 2
```
#Switch kubectx to Cluster 2
export KUBECONFIG=$CLUSTER_2

#Install test services in Cluster 2

kubectl apply -f $ADMIRAL_HOME/yaml/remotecluster_sample.yaml
```

#### Verify

```
#Switch kubectx to Cluster 1
export KUBECONFIG=$CLUSTER_1

# Verify that the ServiceEntry for greeting service in Cluster 1 now has second endpoint (Cluster 2's istio-ingressgateway address)
kubectl get serviceentry default.greeting.global-se -n admiral-sync -o yaml
```

#### Demo

Now run the below request multiple times and see the requests being load balanced between local (Cluster 1) and remote (Cluster 2) instances of greeting service (You can see the response payload change based on which greeting's instance served the request)

```
kubectl exec --namespace=sample -it $(kubectl get pod -l "app=webapp" --namespace=sample -o jsonpath='{.items[0].metadata.name}') -c webapp -- curl -v http://default.greeting.global
```

### Global traffic policy

Multicluster example is a prerequisite for the below example with Cluster 2 hosted in us-east-2 region.

You can add a global traffic policy for the Greeting service to distribute traffic between clusters in a certain ratio. 

```bash
kubectl apply -f $ADMIRAL_HOME/yaml/gtp.yaml
```

Now, when you re-run demo requests, you should see 80% of them being served from the us-west-2 cluster (Cluster 1) and 20% of them being served from us-east-2 (Cluster 2).

`Note`: You can add locality to your pods in Cluster 2 by using K8s standard region labels if your cluster if not running on a cloud provider like AWS. See these [requirements](https://istio.io/docs/ops/configuration/traffic-management/locality-load-balancing/#requirements)

#### Argo-Rollouts 

#### Install Argo-rollouts
Refer [this](https://argoproj.github.io/argo-rollouts/) for details on Argo-Rollouts

```
kubectl create namespace argo-rollouts

kubectl apply -n argo-rollouts -f https://raw.githubusercontent.com/argoproj/argo-rollouts/stable/manifests/install.yaml
```

#### Deploy Sample Argo-Rollouts Services with Blue-Green stratergy
Refer [this](https://argoproj.github.io/argo-rollouts/features/bluegreen/) for details on Blue-Green stratergy with Argo-Rollouts

```
#Install test services

kubectl apply -f $ADMIRAL_HOME/yaml/sample-greeting-rollout-bluegreen.yaml
```
#### Demo

Now, run the command below that uses the CNAME generated by Admiral
```
kubectl exec --namespace=sample-rollout-bluegreen -it $(kubectl get pod -l "app=webapp" --namespace=sample-rollout-bluegreen -o jsonpath='{.items[0].metadata.name}') -c webapp -- curl -v http://bluegreen.greeting.global

```


#### Deploy Sample Argo-Rollouts Services with Canary stratergy
Refer [this](https://argoproj.github.io/argo-rollouts/features/canary/) for details on Canary stratergy with Argo-Rollouts

```
#Install test services

kubectl apply -f $ADMIRAL_HOME/yaml/sample-greeting-rollout-canary.yaml
```

#### Demo

Now, run the command below that uses the CNAME generated by Admiral
```
kubectl exec --namespace=sample-rollout-canary -it $(kubectl get pod -l "app=webapp" --namespace=sample-rollout-canary -o jsonpath='{.items[0].metadata.name}') -c webapp -- curl -v http://canary.greeting.global

```

### Cleanup

Run the following script to cleanup admiral and its associated resources

```bash
$ADMIRAL_HOME/scripts/cleanup.sh
```
